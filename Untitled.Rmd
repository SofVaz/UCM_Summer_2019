---
title: "ILO summary"
author: "DARS"
date: "6/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Libraries
```{r libraries, include = F}
library(tidyverse)
library(stringr)
library(readxl)
library(tidytext)
library(udpipe) #language model
```
#Introduction
In this section I give a basic overview of our data.
##Import data
```{r import}
ilo_data <- read_excel("~/Documents/Summer UCM/ILO/ILO_2018_2019.xlsx", na = c("", "NA"))
```
```{r add course level and concentration label}
ilo_data <- ilo_data %>% mutate(Level = str_sub(Course, start = 4, end = 4), Concentration = str_sub(Course, start = 1, end = 3))
```

##Number of courses
I have data for: 
```{r number of courses}
no_courses <- ilo_data %>% select(Course) %>% unique() %>% count() %>% pull(n)
print(no_courses)

rm(no_courses)
```
these do not include the projects. 

##Distribution of ILO per course
```{r distribution of ILOs per course}
ilo_data %>% 
  mutate(not_empty = !is.na(ILO)) %>% 
  group_by(Course) %>%
  summarise(Number_ilos = sum(not_empty)) %>%
  count(Number_ilos) %>% 
  ggplot(aes(x = Number_ilos, y = n)) +
  geom_histogram(stat = "identity", fill = "navy") +
  labs(title= "Distribution of ILOs per Course", x = "Number of ILOs", y = "Number of Courses") +
  coord_flip() +
  theme_minimal()
```
###Total number of ILOS
```{r total number of ILOs}
ilo_data %>% 
  transmute(not_empty = !is.na(ILO)) %>% summarize(total_ilos = sum(not_empty))
```

##Course vs Student orientation
ILOs are formulated either as student learning goals or course objectives:
```{r course orientation}
ilo_data %>% select(`Course_Intention/Student_profile`) %>% group_by(`Course_Intention/Student_profile`) %>% count() %>%
  ggplot(aes(x = `Course_Intention/Student_profile`, y = n, label = n)) +
  geom_histogram(stat = "identity", fill = "navy")+
  geom_text(vjust = -0.5) +
  labs(title = "ILO formulation orientation", x = "Course (C) vs Student (S) Orientation", y = "Number of ILOs")+
  theme_classic()
  
```
in percentages:
```{r orientation percentage}
ilo_data %>% select(`Course_Intention/Student_profile`) %>% group_by(`Course_Intention/Student_profile`) %>% count() %>%
  ungroup() %>%
  mutate(percentage = n*100/sum(n)) %>%
  ggplot(aes(x = `Course_Intention/Student_profile`, y = percentage, label = round(percentage))) +
  geom_histogram(stat = "identity", fill = "navy")+
  geom_text(vjust = -0.5) +
  labs(title = "ILO formulation orientation", x = "Course (C) vs Student (S) Orientation", y = "Percentage of ILOs")+
  theme_classic()
  
```

the ILOs I did not know how to classify are:
```{r Orientation unclear}
ilo_data %>% filter(is.na(`Course_Intention/Student_profile`)) %>% select(Course, ILO)
```

##Basic Text analysis
I count the appearance of words accross all courses, and add that number to the dataframe. 
```{r count words}
ilo_unnested <-  ilo_data %>% 
  select(Course, ILO, Level, Concentration) %>% 
  unnest_tokens(word, ILO)

# basic_word_counts <- ilo_unnested %>% 
#   count(word)

# ilo_word_counts <- ilo_unnested %>% left_join(basic_word_counts, by = "word")
# 
# ilo_word_counts
```

Here is the very basic count of words that appear:
```{r basic word count}
basic_word_counts
```
###Classes of words
I will manually remove some of the common words such as prepositions and conjunctions:
```{r my_stopwords}
prepositions <- c(
      "with", "at", "from", "into", "during", "including", "until", "against", "among", "throughout",
      "despite", "towards", "upon", "concerning", "to", "in", "for", "of", "on", "by", "about", "like", "through",
      "over","before", "between", "after","since","without", "under", "within","along","following", "across",
      "behind","beyond", "plus", "except", "but", "up","out", "around", "down", "off", "above", "near"
)

conjunctions <- c("and", "or", "but","nor", "for","yet", "after", "although", "as", "because", "before","even", "once", "since", "though", "till", "unless", "until", "what", "when", "whenever", "wherever", "whether","while")

determiners <- c("this", "that", "these", "those")

articles <- c("the", "a", "an")

pronouns <- c("it", "one", "you", "we", "they",
              "his", "hers", "ours", "their")

my_stopwords <- c(word = c(prepositions, conjunctions, articles, determiners, pronouns))

#Clean workspace
rm(prepositions, conjunctions, articles, determiners, pronouns)
```
removing common words:
```{r remove stopwords}
basic_word_counts %>%
  filter(!str_detect(word,"[0-9]+")) %>% #remove numbers
  filter(! word %in% my_stopwords)
```
###Universal part of speech
Alternatively, we should check out this package, created by the Institute of Formal and Applied Linguistics of Charles University, Czech Republic at the Faculty of Mathematics and Physics. http://ufal.mff.cuni.cz/udpipe 

Lables are as follows: 
ADJ: adjective
ADP: adposition
ADV: adverb
AUX: auxiliary
CCONJ: coordinating conjunction
DET: determiner
INTJ: interjection
NOUN: noun
NUM: numeral
PART: particle
PRON: pronoun
PROPN: proper noun
PUNCT: punctuation
SCONJ: subordinating conjunction
SYM: symbol
VERB: verb
X: other
```{r language model}
# library(udpipe)

english_lang_model <- udpipe_download_model(language = "english")
udmodel_english <- udpipe_load_model(file = english_lang_model$file_model)
s <- udpipe_annotate(udmodel_english, ilo_unnested$word)
x <- data.frame(s) %>% select(token, upos, xpos) %>% distinct()

#rm(english_lang_model, udmodel_english, s)
```
Annotating the data based on the language model:
```{r annotate data}
annotated_ilo <- ilo_unnested %>% left_join(x, by = c("word" = "token"))
```
Filtering for verbs:
```{r verbs in all courses}
verbs_in_ilos <- annotated_data %>%
  filter(upos == "VERB") 
```
Universal Verb counts
```{r verb counts}
verbs_unrepeated <- verbs_in_ilos %>%
  group_by(word) %>%
  mutate(n = n()) %>%
  select(word, n) %>%
  distinct()

verbs_unrepeated %>%
  top_n(n = 20, wt = n) %>%
  ungroup() %>%
  .[1:20,] %>%
  ggplot(aes(x = reorder(word,-n), y = n))+
  geom_histogram(stat = "identity", fill = "navy")+
  labs(title = "Most Common Verbs", x = "Verb", y = "Number of appearances")+
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

####Breakdown by Level
```{r level break down}
verbs_unrepeated <- verbs_in_ilos %>%
  group_by(Level, word) %>%
  mutate(n = n()) %>%
  select(Level, word, n) %>%
  distinct()

d_level_plots <- verbs_unrepeated %>% 
  group_by(Level) %>%
  top_n(n = 20, wt = n) %>%
  ungroup()

#Level 1000
d_level_plots %>%
  filter(Level == 1) %>%
  ggplot(aes(x = reorder(word,-n), y = n)) +
  geom_histogram(stat = "identity", fill = "navy") +
  labs(title = "Most Common Verbs for 1000 Level", x = "Verb", y = "Number of appearances")+
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
#Level2000
d_level_plots %>%
  filter(Level == 2) %>%
  ggplot(aes(x = reorder(word,-n), y = n)) +
  geom_histogram(stat = "identity", fill = "navy") +
  labs(title = "Most Common Verbs for 2000 Level", x = "Verb", y = "Number of appearances")+
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

#Level3000
d_level_plots %>%
  filter(Level == 3) %>%
  ggplot(aes(x = reorder(word,-n), y = n)) +
  geom_histogram(stat = "identity", fill = "navy") +
  labs(title = "Most Common Verbs for 3000 Level", x = "Verb", y = "Number of appearances")+
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

#Manual analysis display
For each of the ILOs the verb describing what the student is intended to learn was extracted manually. This created a distiction between the course verb and the student verb. For instance, for the ILO "to provide students with [...] perspectives to examine..." (COR1004, p.1) the extracted verb is "[to] examine".
 
Let's have a look at all the verbs I extracted.

First, I clean the data to have all verbs in one column. I replace `NA` with a character (`"NaN"`) to be able to remove `NA`s created by `gather`, after those have been filtered out, I return `"NaN"` to `NA`.
```{r clean ilo data}
ilo_data_clean <- ilo_data %>% 
  mutate(Verb_used_1 = case_when(is.na(Verb_used_1)~"NaN",
                                            T~Verb_used_1))      %>%
  gather(key = tmp_place, value = Verb, Verb_used_1:Verd_used_3) %>% 
  select(-tmp_place) %>% 
  filter(!is.na(Verb)) %>% 
  mutate(Verb = map_chr(Verb, function(x) str_replace(x,"NaN",NA_character_)))
```

```{r dirty verbs}
dirty_verbs <- ilo_data_clean %>% pull(Verb) %>% unique()
dirty_verbs
```
as you can see some verbs are really similar, for example, we have: "to understand", "understand" and "basic understanding". Let's group all of these together.

We will need an or function to help with regular expressions:
```{r collapse or}
collapse_or <- function(string_vector){
  paste(string_vector, collapse ="$|^")
}
```

First find similarities
```{r search for equivalence}
#UNDERSTAND
understand_eq <- ilo_data_clean %>% filter(str_detect(Verb, "understa")) %>% pull(Verb) %>% unique()
print(paste('"understand":', paste(understand_equivalence, collapse = ", "), sep = " "))

#KNOW
know_eq <- ilo_data_clean %>% filter(str_detect(Verb, "know")) %>% pull(Verb) %>% unique()
print(paste('"know":', paste(know_equivalence, collapse = ", "), sep = " ")) # "gain "basic practial knowledge" and "use knowledge" might not be correct here rather in "apply".

#[GAIN] FAMILIARITY
familiarise_eq <- ilo_data_clean %>% filter(str_detect(Verb, "familia")) %>% pull(Verb) %>% unique()
print(paste('"[gain] familiarity":', paste(familiarise_equivalence, collapse = ", "), sep = " "))

#ANALYSE
analyse_eq <- ilo_data_clean %>% filter(str_detect(Verb, "analy")) %>% pull(Verb) %>% unique()
print(paste('"analyse":', paste(analyse_equivalence, collapse = ", "), sep = " "))

#[BE] INTRODUCED
introduce_eq <- ilo_data_clean %>% filter(str_detect(Verb, "introd")) %>% pull(Verb) %>% unique()
print(paste('"[be] introduced":', paste(introduce_equivalence, collapse = ", "), sep = " "))

#[GET] OVERVIEW
overview_eq <- ilo_data_clean %>% filter(str_detect(Verb, "overv")) %>% pull(Verb) %>% unique()
print(paste('"[get] overview":', paste(overview_eq, collapse = ", "), sep = " "))

#[GET] ACQUAINTED
acquainted_eq <- ilo_data_clean %>% filter(str_detect(Verb, "acquain")) %>% pull(Verb) %>% unique()
print(paste('"[get] acquainted":', paste(acquainted_eq, collapse = ", "), sep = " "))
```
NOTES:
 * "gain basic practial knowledge" and "use knowledge" might not be correct under "know" rather in "apply". 
 * "think analytically" and "reason analyticaly" don't have a clear interpretation to me. 

We notice that some of the words recognized by are not quite equivalent. Therefore, I create equivalent classes for filtering:
```{r equivanlent classes}
#UNDERSTAND
understand_equivalence <- setdiff(understand_eq, c(""))
print(paste('The verbs that were taken to be the same as "understand" are:', paste(understand_equivalence, collapse = ", "), sep = " "))

#KNOW
know_equivalence <- setdiff(know_eq, c("use knowledge","gain basic practical knowledge"))
print(paste('The verbs that were taken to be the same as "know" are:', paste(know_equivalence, collapse = ", "), sep = " ")) # "gain "basic practial knowledge" and "use knowledge" might not be correct here rather in "apply".

#[GAIN] FAMILIARITY
familiarise_equivalence <- setdiff(familiarise_eq, c(""))
print(paste('The verbs that were taken to be the same as "[gain] familiarity" are:', paste(familiarise_equivalence, collapse = ", "), sep = " "))

#ANALYSE
analyse_equivalence <- setdiff(analyse_eq, c("to further (research, analyical and writing skills)", "define/analyse/answer", "reason analyticaly", "think analytically"))
print(paste('The verbs that were taken to be the same as "analyse" are:', paste(analyse_equivalence, collapse = ", "), sep = " "))

#[BE] INTRODUCED
introduce_equivalence <- setdiff(introduce_eq, c(""))
print(paste('The verbs that were taken to be the same as "[be] introduced" are:', paste(introduce_equivalence, collapse = ", "), sep = " "))

#[GET] OVERVIEW
overview_equivalence <- setdiff(overview_eq, c(""))
print(paste('The verbs that were taken to be the same as "[get] overview" are:', paste(overview_equivalence, collapse = ", "), sep = " "))

#[GET] ACQUAINTED
acquainted_equivalence <- setdiff(acquainted_eq, c(""))
print(paste('The verbs that were taken to be the same as "[get] acquainted" are:', paste(acquainted_equivalence, collapse = ", "), sep = " "))

```
Next, I create standardizing functions that use those equivalent classes to replace all verbs in the same class for a single verb.
```{r cleaning other verbs}
#Clean functions
remove_to <- function(string){
  str_replace(string, "to ", "")
}

#Standardize functions
standardize_knowing <- function(string){
  str_replace(string, collapse_or(know_equivalence), "know")
}

standardize_understand <- function(string){
  str_replace(string, collapse_or(understand_equivalence), "understand")
}

standardize_familiarity <- function(string){
  str_replace(string, collapse_or(familiarise_equivalence), "[gain] familiarity")
}

standardize_analyse <- function(string){
  str_replace(string, collapse_or(analyse_equivalence), "analyse")
}

standardize_introduce <- function(string){
  str_replace(string, collapse_or(introduce_equivalence), "[be] introduced")
}

standardize_overview <- function(string){
  str_replace(string, collapse_or(overview_equivalence), "[get] overview")
}

standardize_acquainted <- function(string){
  str_replace(string, collapse_or(acquainted_equivalence), "[get] acquainted")
}
```
Finally, I standardize the data by applying our functions so all verbs in equivalent classes are mapped to the same verb. Now, our data is clean.
```{r apply standarization}
ilo_data_clean <- ilo_data_clean %>% 
  mutate(
    Verb = map_chr(Verb, standardize_knowing),
    Verb = map_chr(Verb, standardize_understand),
    Verb = map_chr(Verb, standardize_familiarity),
    Verb = map_chr(Verb, standardize_analyse),
    Verb = map_chr(Verb, standardize_introduce),
    Verb = map_chr(Verb, standardize_overview),
    Verb = map_chr(Verb, standardize_acquainted),
    Verb = map_chr(Verb, function(string) str_replace(string, "to ", "")) #,
    #Verb = map_chr(Verb, function(string) str_replace(string, "^na$", NA_character_)) #this should not be necessary anymore.
  ) 
```
```{r}
ilo_data_clean %>% View()
```

